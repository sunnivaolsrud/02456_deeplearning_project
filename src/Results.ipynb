{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "import torch\n",
    "import ast \n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains all results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the LSTM model\n",
    "(Her mangler vi dataen figurerne bliver lavet ud fra.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1,num_epochs+1), y=train_loss_list, color='blue', label='Train');\n",
    "sns.lineplot(x=np.arange(1, num_epochs + 1), y=valid_loss_list, color='orange', label='Valid');\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"50epochs_cleaned.png\")\n",
    "\n",
    "# %%\n",
    "test_loss, test_acc, test_y_pred, test_y_true, test_y_pred_round = evaluate_model(model, test_iter, optimizer)\n",
    "test_y_pred_cat = torch.cat(test_y_pred)\n",
    "test_y_true_cat = torch.cat(test_y_true)\n",
    "test_auc = get_auroc(test_y_true_cat, test_y_pred_cat)\n",
    "test_spear = spearman(test_y_true_cat,test_y_pred_cat)\n",
    "print(f'''Test AUC score: {test_auc:.3f}''')\n",
    "print(test_spear)\n",
    "\n",
    "# Roc curve\n",
    "fpr, tpr, threshold = roc_curve(test_y_true_cat, test_y_pred_cat)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.title('ROC LSTM')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\")\n",
    "plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig(\"50-epochs-roc-cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new predictions by loading pretrained model and tokenizer \n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification, Trainer\n",
    "# from datasets import load_from_disk\n",
    "# import evaluate\n",
    "\n",
    "# Load Tokenizer and model for predicting\n",
    "# tokenized_imdb = load_from_disk(\"src/data/tokenized_data\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#              \"src/bert_models/trained_bert_3\", num_labels=2)\n",
    "# trainer = Trainer(model)\n",
    "\n",
    "# %% Prediction \n",
    "# testset = tokenized_imdb[\"Validation\"]\n",
    "# predictions, labels, _, = trainer.predict(testset)\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# print(metric.compute(predictions=np.argmax(predictions,-1), references=labels))\n",
    "\n",
    "#%% Save/load predictions\n",
    "# np.save('src/data/PredictedTestData/PredictionTestData',predictions)\n",
    "# np.save('src/data/PredictedTestData/Labels',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load earlier prediction\n",
    "predictions = np.load('src/data/PredictedTestData/PredictionTestData.npy')\n",
    "labels      = np.load('src/data/PredictedTestData/Labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy and examples\n",
    "#accuarcy\n",
    "print(np.sum(np.argmax(predictions,axis=-1)==labels)/200)\n",
    "#examples\n",
    "print(testset['Tweet'][:2], testset['Tweet'][175],testset['Tweet'][45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "exppred = np.exp(predictions)\n",
    "predsoft = exppred/np.sum(exppred)\n",
    "predsoft /= np.max(predsoft)\n",
    "ratio = np.c_[[(predsoft[:,1]>i) for i in np.linspace(0.01,0.95,1000)]]\n",
    "\n",
    "TP = np.sum((labels==ratio)[:,labels==1], 1)\n",
    "TN = np.sum((labels==ratio)[:,labels==0], 1)\n",
    "FP = np.sum((labels!=ratio)[:,labels==0], 1) \n",
    "FN = np.sum((labels!=ratio)[:,labels==1], 1)\n",
    "\n",
    "TPR = TP/(TP+FN) # Recall/sensitivity\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.title('ROC BERT')\n",
    "plt.plot(FPR, TPR)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig(\"50-epochs-roc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score\n",
    "auc(FPR, TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loss\n",
    "\n",
    "# load data from output file\n",
    "output = []\n",
    "for i in ['Output_14988963']:\n",
    "    with open(f'src/log/{i}.out') as f:\n",
    "        output.append(np.array(f.readlines())[1:])\n",
    "messagedict =  [list(map(ast.literal_eval,run)) for run in output]\n",
    "\n",
    "mapp = lambda x: [[epoch[x] for epoch in run if x in epoch] for run in messagedict]\n",
    "eval_loss=mapp('eval_loss');eval_accu=mapp('eval_accuracy');train_loss=mapp('loss') \n",
    "\n",
    "# visualize train and validation loss\n",
    "num_epochs = 10\n",
    "train_loss_ = np.array(train_loss[0])[np.linspace(0,len(train_loss[0])-1,10).astype(int)]\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1,num_epochs+1), y=train_loss_, color='blue', label='Train');\n",
    "sns.lineplot(x=np.arange(1, num_epochs + 1), y=eval_loss[0], color='orange', label='Valid');\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HRnet_Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1dca15f1148ce851678aad6e1f99401e4ba5411c7555a736b6fd2e38fc8fb9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
