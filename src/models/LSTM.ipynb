{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data import Field, LabelField, BucketIterator, TabularDataset\n",
    "from torchtext.vocab import Vocab\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from scipy.stats import spearmanr\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sunniva/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=105, hidden_size=179, output_dim=1, dropout_rate=0.58,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(LSTM_model, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size, **kwargs)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, tensor_batch):\n",
    "\n",
    "        embedding_tensor = self.embedding(tensor_batch)\n",
    "\n",
    "        dropout_embedding = self.dropout(embedding_tensor)\n",
    "\n",
    "        out, (hidden_state, _) = self.lstm(dropout_embedding)\n",
    "\n",
    "        hidden_squeezed = hidden_state.squeeze(0)\n",
    "\n",
    "        assert torch.equal(out[-1, :, :], hidden_squeezed)\n",
    "\n",
    "        return self.linear(hidden_squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iter, optimizer):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iter:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.Tweet).squeeze(1) # removing the extra dimension ([batch_size,1])\n",
    "\n",
    "        loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss = loss_function(predictions, batch.overall_label)  # batch loss\n",
    "\n",
    "        predicted_classes = torch.round(torch.sigmoid(predictions))\n",
    "\n",
    "        correct_preds = (predicted_classes == batch.overall_label).float()\n",
    "\n",
    "        accuracy = correct_preds.sum() / len(correct_preds)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # add the loss for this batch to calculate the loss for whole epoch\n",
    "        epoch_acc += accuracy.item()  # .item() tend to give the exact number from the tensor of shape [1,]\n",
    "\n",
    "\n",
    "\n",
    "    return epoch_loss / len(train_iter), epoch_acc / len(train_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_test_iter, optimizer):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    # Two lists are used to calculate AUC score\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_round = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in val_test_iter:\n",
    "            predictions = model(batch.Tweet).squeeze(1)\n",
    "\n",
    "            loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "            loss = loss_function(predictions, batch.overall_label)\n",
    "\n",
    "            predicted_classes = torch.sigmoid(predictions)\n",
    "            y_pred.append(predicted_classes)\n",
    "\n",
    "            pred_classes = torch.round(torch.sigmoid(predictions))\n",
    "            y_pred_round.append(pred_classes)\n",
    "\n",
    "            correct_predictions = (pred_classes == batch.overall_label).float()\n",
    "\n",
    "            accuracy = correct_predictions.sum() / len(correct_predictions)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy.item()\n",
    "            y_true.append(batch.overall_label)\n",
    "\n",
    "        return total_loss / len(val_test_iter), total_acc / len(val_test_iter), y_pred, y_true, y_pred_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auroc(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    auc = roc_auc_score(truth.numpy(), pred.numpy())\n",
    "    return auc\n",
    "\n",
    "def spearman(x,y):\n",
    "    return spearmanr(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "source_folder = '../data/twitter_data/'\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.determinstic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use 'cuda' if available else 'cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = Field(tokenize=word_tokenize)\n",
    "# tokenize text using word_tokenize and convert to numerical form using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelField(dtype=torch.float)\n",
    "# useful for label string to LabelEncoding. Not useful here but doesn't hurt either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('Tweet', tweet), ('overall_label', label)]\n",
    "# (column name,field object to use on that column) pair for the dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = TabularDataset.splits(path=source_folder, train='overall_label_dataset_train.csv', test='overall_label_dataset_test.csv',\n",
    "                                             format='csv', fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.build_vocab(train)\n",
    "label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = BucketIterator.splits((train, test), batch_sizes=(64, 64),\n",
    "                                                            sort_key=lambda x: len(x.tweet),\n",
    "                                                            sort_within_batch=False,\n",
    "                                                            device=device)  # use the cuda device if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tweet.vocab)\n",
    "lr = 3e-4  # learning rate = 0.0003\n",
    "model = LSTM_model(vocab_size)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Example' object has no attribute 'tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         train_loss, train_acc \u001b[39m=\u001b[39m train_model(model, train_iter, optimizer)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         testation_loss, testation_acc, y_pred, y_true, test_y_pred \u001b[39m=\u001b[39m evaluate_model(model, test_iter, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         test_loss_list\u001b[39m.\u001b[39mappend(testation_loss)\n",
      "\u001b[1;32m/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb Cell 18\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_test_iter, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m val_test_iter:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         predictions \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39mTweet)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         loss_function \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/iterator.py:141\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_epoch()\n\u001b[1;32m    142\u001b[0m         \u001b[39mfor\u001b[39;00m idx, minibatch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches):\n\u001b[1;32m    143\u001b[0m             \u001b[39m# fast-forward if loaded from state\u001b[39;00m\n\u001b[1;32m    144\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterations_this_epoch \u001b[39m>\u001b[39m idx:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/iterator.py:117\u001b[0m, in \u001b[0;36mIterator.init_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_state_this_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_shuffler\u001b[39m.\u001b[39mrandom_state\n\u001b[0;32m--> 117\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_batches()\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restored_from_state:\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restored_from_state \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/iterator.py:245\u001b[0m, in \u001b[0;36mBucketIterator.create_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort:\n\u001b[0;32m--> 245\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches \u001b[39m=\u001b[39m batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    246\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size_fn)\n\u001b[1;32m    247\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches \u001b[39m=\u001b[39m pool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    249\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size_fn,\n\u001b[1;32m    250\u001b[0m                             random_shuffler\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_shuffler,\n\u001b[1;32m    251\u001b[0m                             shuffle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffle,\n\u001b[1;32m    252\u001b[0m                             sort_within_batch\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_within_batch)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/iterator.py:102\u001b[0m, in \u001b[0;36mIterator.data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort:\n\u001b[0;32m--> 102\u001b[0m     xs \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, key\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort_key)\n\u001b[1;32m    103\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffle:\n\u001b[1;32m    104\u001b[0m     xs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_shuffler(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset)))]\n",
      "\u001b[1;32m/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb Cell 18\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_iter, test_iter \u001b[39m=\u001b[39m BucketIterator\u001b[39m.\u001b[39msplits((train, test), batch_sizes\u001b[39m=\u001b[39m(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                                             sort_key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39;49mtweet),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                             sort_within_batch\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                             device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'tweet'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_model(model, train_iter, optimizer)\n",
    "        testation_loss, testation_acc, y_pred, y_true, test_y_pred = evaluate_model(model, test_iter, optimizer)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(testation_loss)\n",
    "        print(\n",
    "            f'''End of Epoch: {epoch + 1}  |  Train Loss: {train_loss:.3f}  |  testation Loss: {testation_loss:.3f}  |  Train Acc: {train_acc * 100:.2f}%  |  testation Acc: {testation_acc * 100:.2f}% ''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('uni_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7634c8171feac4458a13f394e6cfdd814bf66fc1aec0214c1deb01e1bca547b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
