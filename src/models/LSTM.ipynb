{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data import Field, LabelField, BucketIterator, TabularDataset\n",
    "from torchtext.vocab import Vocab\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from scipy.stats import spearmanr\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sunniva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=105, hidden_size=179, output_dim=1, dropout_rate=0.58,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(LSTM_model, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size, **kwargs)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, tensor_batch):\n",
    "\n",
    "        embedding_tensor = self.embedding(tensor_batch)\n",
    "\n",
    "        dropout_embedding = self.dropout(embedding_tensor)\n",
    "\n",
    "        out, (hidden_state, _) = self.lstm(dropout_embedding)\n",
    "\n",
    "        hidden_squeezed = hidden_state.squeeze(0)\n",
    "\n",
    "        assert torch.equal(out[-1, :, :], hidden_squeezed)\n",
    "\n",
    "        return self.linear(hidden_squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iter, optimizer):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iter:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.Tweet).squeeze(1) # removing the extra dimension ([batch_size,1])\n",
    "\n",
    "        loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss = loss_function(predictions, batch.overall_label)  # batch loss\n",
    "\n",
    "        predicted_classes = torch.round(torch.sigmoid(predictions))\n",
    "\n",
    "        correct_preds = (predicted_classes == batch.overall_label).float()\n",
    "\n",
    "        accuracy = correct_preds.sum() / len(correct_preds)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # add the loss for this batch to calculate the loss for whole epoch\n",
    "        epoch_acc += accuracy.item()  # .item() tend to give the exact number from the tensor of shape [1,]\n",
    "\n",
    "\n",
    "\n",
    "    return epoch_loss / len(train_iter), epoch_acc / len(train_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_test_iter, optimizer):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    # Two lists are used to calculate AUC score\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_round = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in val_test_iter:\n",
    "            predictions = model(batch.Tweet).squeeze(1)\n",
    "\n",
    "            loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "            loss = loss_function(predictions, batch.overall_label)\n",
    "\n",
    "            predicted_classes = torch.sigmoid(predictions)\n",
    "            y_pred.append(predicted_classes)\n",
    "\n",
    "            pred_classes = torch.round(torch.sigmoid(predictions))\n",
    "            y_pred_round.append(pred_classes)\n",
    "\n",
    "            correct_predictions = (pred_classes == batch.overall_label).float()\n",
    "\n",
    "            accuracy = correct_predictions.sum() / len(correct_predictions)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy.item()\n",
    "            y_true.append(batch.overall_label)\n",
    "\n",
    "        return total_loss / len(val_test_iter), total_acc / len(val_test_iter), y_pred, y_true, y_pred_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auroc(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    auc = roc_auc_score(truth.numpy(), pred.numpy())\n",
    "    return auc\n",
    "\n",
    "def spearman(x,y):\n",
    "    return spearmanr(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "source_folder = '../data/twitter_data/'\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.determinstic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use 'cuda' if available else 'cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet = Field(tokenize=word_tokenize)\n",
    "# tokenize text using word_tokenize and convert to numerical form using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelField(dtype=torch.float)\n",
    "# useful for label string to LabelEncoding. Not useful here but doesn't hurt either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('Tweet', Tweet), ('overall_label', label)]\n",
    "# (column name,field object to use on that column) pair for the dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = TabularDataset.splits(path=source_folder, train='overall_label_dataset_train.csv', test='overall_label_dataset_test.csv',\n",
    "                                             format='csv',skip_header=True, fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet.build_vocab(train)\n",
    "label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = BucketIterator.splits((train, test), batch_sizes=(64, 64),\n",
    "                                                            sort_key=lambda x: len(x.Tweet),\n",
    "                                                            sort_within_batch=False,\n",
    "                                                            device=device)  # use the cuda device if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(Tweet.vocab)\n",
    "lr = 3e-4  # learning rate = 0.0003\n",
    "model = LSTM_model(vocab_size)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hellyeahfahmi damn tho perfect timing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         train_loss, train_acc \u001b[39m=\u001b[39m train_model(model, train_iter, optimizer)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         test_loss, test_acc, y_pred, y_true, test_y_pred \u001b[39m=\u001b[39m evaluate_model(model, test_iter, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         test_loss_list\u001b[39m.\u001b[39mappend(test_loss)\n",
      "\u001b[1;32m/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb Cell 18\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_test_iter, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m val_test_iter:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         predictions \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39mTweet)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sunniva/University/02456_Deeplearning/02456_deeplearning_project/src/models/LSTM.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         loss_function \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/iterator.py:156\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m             minibatch\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_key, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 156\u001b[0m     \u001b[39myield\u001b[39;00m Batch(minibatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepeat:\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     batch \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(x, name) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m---> 34\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, field\u001b[39m.\u001b[39;49mprocess(batch, device\u001b[39m=\u001b[39;49mdevice))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/field.py:237\u001b[0m, in \u001b[0;36mField.process\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[39mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m padded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(batch)\n\u001b[0;32m--> 237\u001b[0m tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumericalize(padded, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/field.py:338\u001b[0m, in \u001b[0;36mField.numericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    336\u001b[0m     arr \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ex] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m arr]\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     arr \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arr]\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing(arr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/uni_env/lib/python3.10/site-packages/torchtext/data/field.py:338\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    336\u001b[0m     arr \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ex] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m arr]\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     arr \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arr]\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing(arr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hellyeahfahmi damn tho perfect timing'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_model(model, train_iter, optimizer)\n",
    "        test_loss, test_acc, y_pred, y_true, test_y_pred = evaluate_model(model, test_iter, optimizer)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        print(\n",
    "            f'''End of Epoch: {epoch + 1}  |  Train Loss: {train_loss:.3f}  |  test Loss: {test_loss:.3f}  |  Train Acc: {train_acc * 100:.2f}%  |  test Acc: {test_acc * 100:.2f}% ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('uni_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7634c8171feac4458a13f394e6cfdd814bf66fc1aec0214c1deb01e1bca547b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
